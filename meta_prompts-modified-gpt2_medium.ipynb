{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301425c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment 1: Baseline with gpt2-medium\n",
      "Dataset: ./Data/gpt2_with_questions_merged.json\n",
      "Sample size: 100\n",
      "Batch size: 16\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: BASELINE (NO COUNTERFACTUAL) - GPT2-MEDIUM\n",
      "======================================================================\n",
      "Loading gpt2-medium on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "âœ“ gpt2-medium loaded successfully\n",
      "  Parameters: 406,236,241\n",
      "  Layers: 24\n",
      "  Hidden size: 1024\n",
      "  Heads: 16\n",
      "âœ“ Dataset loaded: 28953 total prompts\n",
      "\n",
      "Dataset Distribution:\n",
      "----------------------------------------\n",
      "  Redefine       : 4329 prompts\n",
      "  Assess         : 4924 prompts\n",
      "  Fact Check     : 4916 prompts\n",
      "  Review         : 4942 prompts\n",
      "  Validate       : 4915 prompts\n",
      "  Verify         : 4927 prompts\n",
      "\n",
      "Processing 100 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Processed 100 prompts successfully\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 1 RESULTS - BASELINE\n",
      "============================================================\n",
      "\n",
      "Overall Metrics:\n",
      "  Total prompts analyzed: 100\n",
      "  Factual predictions: 100 (100.0%)\n",
      "  Counterfactual predictions: 0 (0.0%)\n",
      "\n",
      "Average Log Probabilities (mean Â± std):\n",
      "  logp(fact): -8.1685 Â± 1.7179\n",
      "  logp(cf):   -12.8848 Â± 2.6871\n",
      "  Î”:          4.7162 Â± 2.2637 (logp(fact) - logp(cf))\n",
      "\n",
      "============================================================\n",
      "ANALYSIS BY PREMISE VERB (PV)\n",
      "============================================================\n",
      "\n",
      "Premise Verb    Count    %Factual   %CF        Avg Î”      Std Î”     \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 682\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;66;03m# Run experiment\u001b[39;00m\n\u001b[32m    681\u001b[39m experiment = BaselineExperiment(DATASET_PATH, MODEL_NAME)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m results = experiment.run_experiment(sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE)\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# Save detailed results\u001b[39;00m\n\u001b[32m    685\u001b[39m experiment.save_results()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 283\u001b[39m, in \u001b[36mBaselineExperiment.run_experiment\u001b[39m\u001b[34m(self, sample_size, batch_size)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prompts successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28mself\u001b[39m.analyze_results()\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.plot_results()\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mBaselineExperiment.analyze_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    331\u001b[39m verb_stats = {}\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m verb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.premise_verbs:\n\u001b[32m    333\u001b[39m     verb_results = [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results \n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(verb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(r.subject) \u001b[38;5;129;01mor\u001b[39;00m verb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(r.question) \n\u001b[32m    335\u001b[39m                         \u001b[38;5;129;01mor\u001b[39;00m verb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(r.factual_answer) \u001b[38;5;129;01mor\u001b[39;00m verb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(r.counterfactual_answer))]\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# Also check if verb appears in any text field\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verb_results:\n",
      "\u001b[31mTypeError\u001b[39m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class BaselineResult:\n",
    "    \"\"\"Stores results for a single prompt\"\"\"\n",
    "    subject: str\n",
    "    question: str\n",
    "    factual_answer: str\n",
    "    counterfactual_answer: str\n",
    "    factual_tokens: List[int]\n",
    "    counterfactual_tokens: List[int]\n",
    "    factual_logp: float\n",
    "    counterfactual_logp: float\n",
    "    delta: float\n",
    "    prediction: str  # \"factual\" or \"counterfactual\"\n",
    "\n",
    "class BaselineExperiment:\n",
    "    def __init__(self, dataset_path: str, model_name: str = \"gpt2-medium\"):\n",
    "        self.model_name = model_name\n",
    "        self.dataset_path = dataset_path\n",
    "        self.model = None\n",
    "        self.dataset = []\n",
    "        self.results = []\n",
    "        \n",
    "        # Premise verbs to analyze\n",
    "        self.premise_verbs = ['Redefine', 'Assess', 'Fact Check', 'Review', 'Validate', 'Verify']\n",
    "        \n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize GPT2-Medium model\"\"\"\n",
    "        try:\n",
    "            self.clear_memory()\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            \n",
    "            print(f\"Loading {self.model_name} on {device}...\")\n",
    "            \n",
    "            # Load with appropriate settings for GPT2-Medium\n",
    "            self.model = HookedTransformer.from_pretrained(\n",
    "                self.model_name,\n",
    "                device=device,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                n_devices=1\n",
    "            )\n",
    "            \n",
    "            # Set model to evaluation mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Print model info\n",
    "            print(f\"âœ“ {self.model_name} loaded successfully\")\n",
    "            print(f\"  Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "            print(f\"  Layers: {self.model.cfg.n_layers}\")\n",
    "            print(f\"  Hidden size: {self.model.cfg.d_model}\")\n",
    "            print(f\"  Heads: {self.model.cfg.n_heads}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error loading {self.model_name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load and filter dataset for baseline experiment\"\"\"\n",
    "        try:\n",
    "            with open(self.dataset_path, 'r') as f:\n",
    "                self.dataset = json.load(f)\n",
    "            \n",
    "            print(f\"âœ“ Dataset loaded: {len(self.dataset)} total prompts\")\n",
    "            \n",
    "            # Group by premise verb for analysis\n",
    "            self.verb_groups = {verb: [] for verb in self.premise_verbs}\n",
    "            \n",
    "            for item in self.dataset:\n",
    "                # Extract premise verb from prompt (if available)\n",
    "                if 'premise_verb' in item:\n",
    "                    premise_verb = item['premise_verb']\n",
    "                elif 'prompt' in item and ':' in item['prompt']:\n",
    "                    premise_verb = item['prompt'].split(':')[0].strip()\n",
    "                else:\n",
    "                    premise_verb = 'Unknown'\n",
    "                    \n",
    "                if premise_verb in self.verb_groups:\n",
    "                    self.verb_groups[premise_verb].append(item)\n",
    "                elif premise_verb != 'Unknown':\n",
    "                    # Add new verb to list if not already present\n",
    "                    self.premise_verbs.append(premise_verb)\n",
    "                    self.verb_groups[premise_verb] = [item]\n",
    "            \n",
    "            print(\"\\nDataset Distribution:\")\n",
    "            print(\"-\" * 40)\n",
    "            total_grouped = 0\n",
    "            for verb in self.premise_verbs:\n",
    "                count = len(self.verb_groups.get(verb, []))\n",
    "                if count > 0:\n",
    "                    print(f\"  {verb:15}: {count:4d} prompts\")\n",
    "                    total_grouped += count\n",
    "            \n",
    "            if total_grouped < len(self.dataset):\n",
    "                print(f\"  {'Uncategorized':15}: {len(self.dataset) - total_grouped:4d} prompts\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error loading dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_baseline_prompt(self, question: str) -> str:\n",
    "        \"\"\"Create baseline prompt: Q Answer: \"\"\"\n",
    "        # Clean the question and add proper formatting\n",
    "        question = question.strip()\n",
    "        if not question.endswith('?'):\n",
    "            question = question + '?'\n",
    "        return f\"{question} Answer:\"\n",
    "    \n",
    "    def tokenize_answer(self, answer: str) -> List[int]:\n",
    "        \"\"\"Tokenize answer into token IDs\"\"\"\n",
    "        # Clean the answer and tokenize\n",
    "        answer = answer.strip()\n",
    "        tokens = self.model.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        return tokens\n",
    "    \n",
    "    def get_log_probabilities(self, prompt: str, target_tokens: List[int]) -> float:\n",
    "        \"\"\"\n",
    "        Compute log probability of target tokens given prompt\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            target_tokens: List of token IDs to compute probability for\n",
    "        \n",
    "        Returns:\n",
    "            Total log probability of the target sequence\n",
    "        \"\"\"\n",
    "        if not target_tokens:\n",
    "            return -float('inf')\n",
    "        \n",
    "        try:\n",
    "            # Tokenize prompt\n",
    "            prompt_tokens = self.model.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            \n",
    "            # Combine prompt and target tokens\n",
    "            all_tokens = prompt_tokens + target_tokens\n",
    "            \n",
    "            # Convert to tensor\n",
    "            tokens_tensor = torch.tensor([all_tokens], device=self.model.cfg.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get logits for all positions\n",
    "                logits = self.model(tokens_tensor)\n",
    "                \n",
    "                # Compute log probabilities using log_softmax\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Extract log probabilities for target tokens\n",
    "                total_logp = 0.0\n",
    "                \n",
    "                for i, token_id in enumerate(target_tokens, start=len(prompt_tokens)):\n",
    "                    # i-1 because logits are shifted by 1 (predicting next token)\n",
    "                    if i-1 >= 0 and i-1 < log_probs.shape[1]:\n",
    "                        token_logp = log_probs[0, i-1, token_id].item()\n",
    "                        total_logp += token_logp\n",
    "                    else:\n",
    "                        # If position is out of bounds, skip\n",
    "                        continue\n",
    "            \n",
    "            return total_logp\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing log probabilities: {e}\")\n",
    "            return -float('inf')\n",
    "    \n",
    "    def process_item(self, item: Dict) -> Optional[BaselineResult]:\n",
    "        \"\"\"Process a single dataset item\"\"\"\n",
    "        try:\n",
    "            # Extract components\n",
    "            question = item['question']\n",
    "            factual_answer = item['target_true']\n",
    "            counterfactual_answer = item['target_new']\n",
    "            subject = item.get('subject', 'Unknown')\n",
    "            \n",
    "            # Create baseline prompt\n",
    "            prompt = self.create_baseline_prompt(question)\n",
    "            \n",
    "            # Tokenize answers\n",
    "            factual_tokens = self.tokenize_answer(factual_answer)\n",
    "            counterfactual_tokens = self.tokenize_answer(counterfactual_answer)\n",
    "            \n",
    "            # Get log probabilities\n",
    "            factual_logp = self.get_log_probabilities(prompt, factual_tokens)\n",
    "            counterfactual_logp = self.get_log_probabilities(prompt, counterfactual_tokens)\n",
    "            \n",
    "            # Compute delta\n",
    "            delta = factual_logp - counterfactual_logp\n",
    "            \n",
    "            # Determine prediction\n",
    "            prediction = \"factual\" if delta > 0 else \"counterfactual\"\n",
    "            \n",
    "            result = BaselineResult(\n",
    "                subject=subject,\n",
    "                question=question,\n",
    "                factual_answer=factual_answer,\n",
    "                counterfactual_answer=counterfactual_answer,\n",
    "                factual_tokens=factual_tokens,\n",
    "                counterfactual_tokens=counterfactual_tokens,\n",
    "                factual_logp=factual_logp,\n",
    "                counterfactual_logp=counterfactual_logp,\n",
    "                delta=delta,\n",
    "                prediction=prediction\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item: {e}\")\n",
    "            print(f\"Item: {item}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "    \n",
    "    def run_experiment(self, sample_size: Optional[int] = None, \n",
    "                      batch_size: int = 32):\n",
    "        \"\"\"Run the baseline experiment with batching for efficiency\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EXPERIMENT 1: BASELINE (NO COUNTERFACTUAL) - {self.model_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if not self.setup_model():\n",
    "            return\n",
    "        \n",
    "        if not self.load_dataset():\n",
    "            return\n",
    "        \n",
    "        # Process all items or sample\n",
    "        all_items = []\n",
    "        for verb in self.premise_verbs:\n",
    "            if verb in self.verb_groups:\n",
    "                all_items.extend(self.verb_groups[verb])\n",
    "        \n",
    "        # Also add items not in verb groups\n",
    "        for item in self.dataset:\n",
    "            if item not in all_items:\n",
    "                all_items.append(item)\n",
    "        \n",
    "        if sample_size:\n",
    "            all_items = all_items[:sample_size]\n",
    "        \n",
    "        print(f\"\\nProcessing {len(all_items)} prompts...\")\n",
    "        \n",
    "        # Process items in batches for efficiency\n",
    "        self.results = []\n",
    "        for i in tqdm(range(0, len(all_items), batch_size), desc=\"Processing batches\"):\n",
    "            batch_items = all_items[i:i+batch_size]\n",
    "            batch_results = []\n",
    "            \n",
    "            for item in batch_items:\n",
    "                result = self.process_item(item)\n",
    "                if result:\n",
    "                    batch_results.append(result)\n",
    "            \n",
    "            self.results.extend(batch_results)\n",
    "            \n",
    "            # Clear memory periodically\n",
    "            if i % (batch_size * 10) == 0 and i > 0:\n",
    "                self.clear_memory()\n",
    "        \n",
    "        print(f\"âœ“ Processed {len(self.results)} prompts successfully\")\n",
    "        \n",
    "        # Analyze results\n",
    "        self.analyze_results()\n",
    "        self.plot_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze and report results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to analyze\")\n",
    "            return\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total = len(self.results)\n",
    "        factual_count = sum(1 for r in self.results if r.prediction == \"factual\")\n",
    "        counterfactual_count = total - factual_count\n",
    "        \n",
    "        factual_percent = (factual_count / total) * 100 if total > 0 else 0\n",
    "        counterfactual_percent = (counterfactual_count / total) * 100 if total > 0 else 0\n",
    "        \n",
    "        # Calculate average log probabilities and delta\n",
    "        avg_factual_logp = np.mean([r.factual_logp for r in self.results]) if self.results else 0\n",
    "        avg_counterfactual_logp = np.mean([r.counterfactual_logp for r in self.results]) if self.results else 0\n",
    "        avg_delta = np.mean([r.delta for r in self.results]) if self.results else 0\n",
    "        \n",
    "        # Calculate standard deviations\n",
    "        std_factual_logp = np.std([r.factual_logp for r in self.results]) if self.results else 0\n",
    "        std_counterfactual_logp = np.std([r.counterfactual_logp for r in self.results]) if self.results else 0\n",
    "        std_delta = np.std([r.delta for r in self.results]) if self.results else 0\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"EXPERIMENT 1 RESULTS - BASELINE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nOverall Metrics:\")\n",
    "        print(f\"  Total prompts analyzed: {total}\")\n",
    "        print(f\"  Factual predictions: {factual_count} ({factual_percent:.1f}%)\")\n",
    "        print(f\"  Counterfactual predictions: {counterfactual_count} ({counterfactual_percent:.1f}%)\")\n",
    "        print(f\"\\nAverage Log Probabilities (mean Â± std):\")\n",
    "        print(f\"  logp(fact): {avg_factual_logp:.4f} Â± {std_factual_logp:.4f}\")\n",
    "        print(f\"  logp(cf):   {avg_counterfactual_logp:.4f} Â± {std_counterfactual_logp:.4f}\")\n",
    "        print(f\"  Î”:          {avg_delta:.4f} Â± {std_delta:.4f} (logp(fact) - logp(cf))\")\n",
    "        \n",
    "        # Analyze by premise verb\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ANALYSIS BY PREMISE VERB (PV)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\n{'Premise Verb':<15} {'Count':<8} {'%Factual':<10} {'%CF':<10} {'Avg Î”':<10} {'Std Î”':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        verb_stats = {}\n",
    "        for verb in self.premise_verbs:\n",
    "            verb_results = [r for r in self.results \n",
    "                          if any(verb in str(r.subject) or verb in str(r.question) \n",
    "                                or verb in str(r.factual_answer) or verb in str(r.counterfactual_answer))]\n",
    "            \n",
    "            # Also check if verb appears in any text field\n",
    "            if not verb_results:\n",
    "                verb_results = [r for r in self.results \n",
    "                              if verb in r.question or verb in r.subject]\n",
    "            \n",
    "            if verb_results:\n",
    "                verb_total = len(verb_results)\n",
    "                verb_factual = sum(1 for r in verb_results if r.prediction == \"factual\")\n",
    "                verb_factual_pct = (verb_factual / verb_total) * 100 if verb_total > 0 else 0\n",
    "                verb_delta_avg = np.mean([r.delta for r in verb_results]) if verb_results else 0\n",
    "                verb_delta_std = np.std([r.delta for r in verb_results]) if verb_results else 0\n",
    "                \n",
    "                verb_stats[verb] = {\n",
    "                    'count': verb_total,\n",
    "                    'factual_pct': verb_factual_pct,\n",
    "                    'counterfactual_pct': 100 - verb_factual_pct,\n",
    "                    'avg_delta': verb_delta_avg,\n",
    "                    'std_delta': verb_delta_std\n",
    "                }\n",
    "                \n",
    "                print(f\"{verb:<15} {verb_total:<8} {verb_factual_pct:<10.1f} \"\n",
    "                      f\"{100-verb_factual_pct:<10.1f} {verb_delta_avg:<10.4f} {verb_delta_std:<10.4f}\")\n",
    "        \n",
    "        return verb_stats\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot experiment results\"\"\"\n",
    "        if not self.results:\n",
    "            return\n",
    "        \n",
    "        # Set style\n",
    "        plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Plot 1: Distribution of Î” values\n",
    "        deltas = [r.delta for r in self.results]\n",
    "        axes[0, 0].hist(deltas, bins=50, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "        axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Î”=0')\n",
    "        axes[0, 0].axvline(x=np.mean(deltas), color='green', linestyle='-', alpha=0.7, linewidth=2, \n",
    "                          label=f'Mean Î”={np.mean(deltas):.2f}')\n",
    "        \n",
    "        # Add normal distribution overlay\n",
    "        from scipy.stats import norm\n",
    "        mu, sigma = np.mean(deltas), np.std(deltas)\n",
    "        x = np.linspace(min(deltas), max(deltas), 100)\n",
    "        axes[0, 0].plot(x, norm.pdf(x, mu, sigma), 'r-', alpha=0.5, label='Normal fit')\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Î” = logp(fact) - logp(cf)', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Density', fontsize=12)\n",
    "        axes[0, 0].set_title(f'Distribution of Î” Values (GPT2-Medium)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].legend(fontsize=10)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Scatter plot of logp(fact) vs logp(cf)\n",
    "        factual_logps = [r.factual_logp for r in self.results]\n",
    "        counterfactual_logps = [r.counterfactual_logp for r in self.results]\n",
    "        \n",
    "        scatter = axes[0, 1].scatter(factual_logps, counterfactual_logps, \n",
    "                                     c=deltas, cmap='RdYlBu', alpha=0.7, \n",
    "                                     edgecolors='black', linewidth=0.3, s=50)\n",
    "        axes[0, 1].plot([min(factual_logps), max(factual_logps)], \n",
    "                       [min(factual_logps), max(factual_logps)], \n",
    "                       'r--', alpha=0.5, linewidth=2, label='y=x (equal prob)')\n",
    "        axes[0, 1].set_xlabel('logp(fact)', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('logp(cf)', fontsize=12)\n",
    "        axes[0, 1].set_title('Factual vs Counterfactual Log Probabilities', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend(fontsize=10)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[0, 1])\n",
    "        cbar.set_label('Î” value', fontsize=12)\n",
    "        \n",
    "        # Plot 3: Prediction distribution\n",
    "        predictions = [r.prediction for r in self.results]\n",
    "        prediction_counts = pd.Series(predictions).value_counts()\n",
    "        colors = ['#4CAF50' if p == 'factual' else '#F44336' for p in prediction_counts.index]\n",
    "        \n",
    "        bars = axes[0, 2].bar(prediction_counts.index, prediction_counts.values, \n",
    "                             color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        axes[0, 2].set_xlabel('Prediction', fontsize=12)\n",
    "        axes[0, 2].set_ylabel('Count', fontsize=12)\n",
    "        axes[0, 2].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add percentage labels and value labels\n",
    "        total = len(self.results)\n",
    "        for i, (bar, (pred, count)) in enumerate(zip(bars, prediction_counts.items())):\n",
    "            percentage = (count / total) * 100\n",
    "            height = bar.get_height()\n",
    "            axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + total*0.005,\n",
    "                           f'{count}\\n({percentage:.1f}%)', \n",
    "                           ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        axes[0, 2].set_ylim(0, max(prediction_counts.values) * 1.15)\n",
    "        \n",
    "        # Plot 4: Î” distribution by premise verb (if available)\n",
    "        premise_deltas = {}\n",
    "        for verb in self.premise_verbs:\n",
    "            verb_deltas = [r.delta for r in self.results \n",
    "                         if verb in r.question or verb in r.subject]\n",
    "            if verb_deltas:\n",
    "                premise_deltas[verb] = verb_deltas\n",
    "        \n",
    "        if premise_deltas:\n",
    "            # Create boxplot\n",
    "            positions = range(1, len(premise_deltas) + 1)\n",
    "            box_data = [premise_deltas[verb] for verb in premise_deltas.keys()]\n",
    "            \n",
    "            bp = axes[1, 0].boxplot(box_data, positions=positions, \n",
    "                                    labels=premise_deltas.keys(), patch_artist=True,\n",
    "                                    medianprops=dict(color='black', linewidth=2),\n",
    "                                    whiskerprops=dict(color='gray', linewidth=1.5),\n",
    "                                    capprops=dict(color='gray', linewidth=1.5))\n",
    "            \n",
    "            # Color boxes based on median Î”\n",
    "            for i, (patch, verb) in enumerate(zip(bp['boxes'], premise_deltas.keys())):\n",
    "                median_val = np.median(premise_deltas[verb])\n",
    "                patch.set_facecolor('#90EE90' if median_val > 0 else '#FFB6C1')\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Premise Verb (PV)', fontsize=12)\n",
    "            axes[1, 0].set_ylabel('Î”', fontsize=12)\n",
    "            axes[1, 0].set_title('Î” Distribution by Premise Verb', fontsize=14, fontweight='bold')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "            axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add sample size annotations\n",
    "            for i, verb in enumerate(premise_deltas.keys()):\n",
    "                count = len(premise_deltas[verb])\n",
    "                axes[1, 0].text(i+1, axes[1, 0].get_ylim()[0] * 0.95, \n",
    "                               f'n={count}', ha='center', va='top', fontsize=9)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No premise verb data\\navailable for plotting', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)\n",
    "            axes[1, 0].set_title('Î” Distribution by Premise Verb', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 5: Cumulative distribution of Î”\n",
    "        sorted_deltas = np.sort(deltas)\n",
    "        cumulative = np.arange(1, len(sorted_deltas) + 1) / len(sorted_deltas)\n",
    "        \n",
    "        axes[1, 1].plot(sorted_deltas, cumulative, 'b-', linewidth=2.5, alpha=0.7, label='CDF')\n",
    "        axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Î”=0')\n",
    "        axes[1, 1].axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # Find and mark median\n",
    "        median_delta = np.median(deltas)\n",
    "        median_idx = np.searchsorted(sorted_deltas, median_delta)\n",
    "        axes[1, 1].plot(median_delta, cumulative[median_idx], 'ro', markersize=10, \n",
    "                       label=f'Median Î”={median_delta:.2f}')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Î” = logp(fact) - logp(cf)', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Cumulative Probability', fontsize=12)\n",
    "        axes[1, 1].set_title('Cumulative Distribution of Î”', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].legend(fontsize=10, loc='lower right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Heatmap of logp correlation\n",
    "        if len(factual_logps) > 0 and len(counterfactual_logps) > 0:\n",
    "            # Create 2D histogram\n",
    "            heatmap, xedges, yedges = np.histogram2d(factual_logps, counterfactual_logps, bins=30)\n",
    "            \n",
    "            # Plot heatmap\n",
    "            im = axes[1, 2].imshow(heatmap.T, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], \n",
    "                                  origin='lower', aspect='auto', cmap='YlOrRd', alpha=0.8)\n",
    "            \n",
    "            # Add diagonal line\n",
    "            min_val = min(min(factual_logps), min(counterfactual_logps))\n",
    "            max_val = max(max(factual_logps), max(counterfactual_logps))\n",
    "            axes[1, 2].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, linewidth=2)\n",
    "            \n",
    "            axes[1, 2].set_xlabel('logp(fact)', fontsize=12)\n",
    "            axes[1, 2].set_ylabel('logp(cf)', fontsize=12)\n",
    "            axes[1, 2].set_title('Density Heatmap of Log Probabilities', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=axes[1, 2])\n",
    "            cbar.set_label('Density', fontsize=12)\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, 'Insufficient data\\nfor heatmap', \n",
    "                           ha='center', va='center', transform=axes[1, 2].transAxes, fontsize=12)\n",
    "            axes[1, 2].set_title('Density Heatmap of Log Probabilities', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(f'GPT2-Medium Baseline Experiment Results\\nExperiment 1: Baseline (No Counterfactual)', \n",
    "                    fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'gpt2_medium_baseline_experiment_{timestamp}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"âœ“ Plot saved as {filename}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Also create a summary figure\n",
    "        self.create_summary_figure(deltas, factual_logps, counterfactual_logps, predictions)\n",
    "    \n",
    "    def create_summary_figure(self, deltas, factual_logps, counterfactual_logps, predictions):\n",
    "        \"\"\"Create a simplified summary figure\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Summary bar chart\n",
    "        factual_count = predictions.count('factual')\n",
    "        cf_count = len(predictions) - factual_count\n",
    "        \n",
    "        bars = axes[0].bar(['Factual', 'Counterfactual'], [factual_count, cf_count], \n",
    "                          color=['#4CAF50', '#F44336'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].set_title('Prediction Summary', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = len(predictions)\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            percentage = (height / total) * 100\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2., height + total*0.01,\n",
    "                        f'{height}\\n({percentage:.1f}%)', \n",
    "                        ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        axes[0].set_ylim(0, max(factual_count, cf_count) * 1.2)\n",
    "        \n",
    "        # Î” statistics box\n",
    "        stats_text = f\"\"\"\n",
    "        Î” Statistics (logp(fact) - logp(cf)):\n",
    "        \n",
    "        Mean Î”: {np.mean(deltas):.4f}\n",
    "        Median Î”: {np.median(deltas):.4f}\n",
    "        Std Î”: {np.std(deltas):.4f}\n",
    "        Min Î”: {np.min(deltas):.4f}\n",
    "        Max Î”: {np.max(deltas):.4f}\n",
    "        \n",
    "        Factual > CF: {sum(1 for d in deltas if d > 0)} prompts\n",
    "        CF > Factual: {sum(1 for d in deltas if d < 0)} prompts\n",
    "        Equal: {sum(1 for d in deltas if d == 0)} prompts\n",
    "        \"\"\"\n",
    "        \n",
    "        axes[1].text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "                    verticalalignment='center', transform=axes[1].transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Statistical Summary', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(f'GPT2-Medium Baseline Experiment - Quick Summary', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'gpt2_medium_baseline_summary_{timestamp}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"âœ“ Summary plot saved as {filename}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def save_results(self, output_path: str = None):\n",
    "        \"\"\"Save detailed results to JSON file\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to save\")\n",
    "            return\n",
    "        \n",
    "        # Create default output path if not provided\n",
    "        if output_path is None:\n",
    "            timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = f'gpt2_medium_baseline_results_{timestamp}.json'\n",
    "        \n",
    "        # Convert results to serializable format\n",
    "        serializable_results = []\n",
    "        for result in self.results:\n",
    "            serializable_results.append({\n",
    "                'subject': result.subject,\n",
    "                'question': result.question,\n",
    "                'factual_answer': result.factual_answer,\n",
    "                'counterfactual_answer': result.counterfactual_answer,\n",
    "                'factual_tokens': result.factual_tokens,\n",
    "                'counterfactual_tokens': result.counterfactual_tokens,\n",
    "                'factual_logp': float(result.factual_logp),\n",
    "                'counterfactual_logp': float(result.counterfactual_logp),\n",
    "                'delta': float(result.delta),\n",
    "                'prediction': result.prediction\n",
    "            })\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        factual_count = sum(1 for r in self.results if r.prediction == \"factual\")\n",
    "        total = len(self.results)\n",
    "        \n",
    "        # Save to file\n",
    "        output_data = {\n",
    "            'experiment': 'Experiment 1: Baseline (No Counterfactual)',\n",
    "            'model': self.model_name,\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'total_prompts': total,\n",
    "            'summary': {\n",
    "                'factual_count': factual_count,\n",
    "                'counterfactual_count': total - factual_count,\n",
    "                'factual_percent': (factual_count / total) * 100 if total > 0 else 0,\n",
    "                'counterfactual_percent': ((total - factual_count) / total) * 100 if total > 0 else 0,\n",
    "                'avg_factual_logp': float(np.mean([r.factual_logp for r in self.results]) if self.results else 0),\n",
    "                'avg_counterfactual_logp': float(np.mean([r.counterfactual_logp for r in self.results]) if self.results else 0),\n",
    "                'avg_delta': float(np.mean([r.delta for r in self.results]) if self.results else 0),\n",
    "                'std_factual_logp': float(np.std([r.factual_logp for r in self.results]) if self.results else 0),\n",
    "                'std_counterfactual_logp': float(np.std([r.counterfactual_logp for r in self.results]) if self.results else 0),\n",
    "                'std_delta': float(np.std([r.delta for r in self.results]) if self.results else 0),\n",
    "            },\n",
    "            'results': serializable_results\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Results saved to {output_path}\")\n",
    "        \n",
    "        # Also save a CSV version for easier analysis\n",
    "        csv_path = output_path.replace('.json', '.csv')\n",
    "        df_data = []\n",
    "        for r in self.results:\n",
    "            df_data.append({\n",
    "                'subject': r.subject,\n",
    "                'question': r.question,\n",
    "                'factual_answer': r.factual_answer,\n",
    "                'counterfactual_answer': r.counterfactual_answer,\n",
    "                'factual_logp': r.factual_logp,\n",
    "                'counterfactual_logp': r.counterfactual_logp,\n",
    "                'delta': r.delta,\n",
    "                'prediction': r.prediction\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ“ CSV results saved to {csv_path}\")\n",
    "\n",
    "# Example usage for GPT2-Medium\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration for GPT2-Medium\n",
    "    DATASET_PATH = \"./Data/gpt2_with_questions_merged.json\"  # Update this path\n",
    "    MODEL_NAME = \"gpt2-medium\"\n",
    "    SAMPLE_SIZE = 100  # Set to None for full dataset, or integer for sampling\n",
    "    BATCH_SIZE = 16  # Adjust based on available GPU memory\n",
    "    \n",
    "    print(f\"Running Experiment 1: Baseline with {MODEL_NAME}\")\n",
    "    print(f\"Dataset: {DATASET_PATH}\")\n",
    "    print(f\"Sample size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Run experiment\n",
    "    experiment = BaselineExperiment(DATASET_PATH, MODEL_NAME)\n",
    "    results = experiment.run_experiment(sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Save detailed results\n",
    "    experiment.save_results()\n",
    "    \n",
    "    # Print example results\n",
    "    if results:\n",
    "        print(\"\\nðŸ“‹ Example Results (first 5 prompts):\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, result in enumerate(experiment.results[:5]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Subject: {result.subject}\")\n",
    "            print(f\"  Question: {result.question}\")\n",
    "            print(f\"  Factual answer: '{result.factual_answer}'\")\n",
    "            print(f\"  Counterfactual answer: '{result.counterfactual_answer}'\")\n",
    "            print(f\"  Prediction: {result.prediction}\")\n",
    "            print(f\"  logp(fact): {result.factual_logp:.4f}\")\n",
    "            print(f\"  logp(cf): {result.counterfactual_logp:.4f}\")\n",
    "            print(f\"  Î”: {result.delta:.4f}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nâœ… Experiment completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a1e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: transformer_lens in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (2.11.0)\n",
      "Requirement already satisfied: pandas in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: seaborn in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scipy in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (1.10.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.3.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (13.7.1)\n",
      "Requirement already satisfied: sentencepiece in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (4.44.0)\n",
      "Collecting typeguard<5.0,>=4.2 (from transformer_lens)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformer_lens) (0.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: psutil in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.7.1->transformer_lens) (3.7)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.11.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.34.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/animesh-lohar-2711/anaconda3/envs/comp_mech_gpu/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: typeguard\n",
      "  Attempting uninstall: typeguard\n",
      "    Found existing installation: typeguard 2.13.3\n",
      "    Uninstalling typeguard-2.13.3:\n",
      "      Successfully uninstalled typeguard-2.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pysvelte 1.0.0 requires typeguard~=2.0, but you have typeguard 4.4.4 which is incompatible.\n",
      "inseq 0.6.0 requires typeguard<=2.13.3, but you have typeguard 4.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typeguard-4.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformer_lens pandas matplotlib numpy tqdm seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970384d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_mech_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
